<!DOCTYPE html>
<html>
<head>
    
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Project Steiner</title>
    <link rel="stylesheet" href="markbind\css\bootstrap.min.css">
    <link rel="stylesheet" href="markbind\css\bootstrap-vue.min.css">
    <link rel="stylesheet" href="markbind\css\font-awesome.min.css" >
    <link rel="stylesheet" href="markbind\css\bootstrap-glyphicons.min.css" >
    <link rel="stylesheet" href="markbind\css\github.min.css">
    <link rel="stylesheet" href="markbind\css\markbind.css">
    <link rel="stylesheet" href="markbind\layouts\default\styles.css">
    <link rel="stylesheet" href="markbind\css\site-nav.css">
    
    <link rel="icon" href="/Steiners_Muse/images/favicon.png">
</head>
<body>
<div id="app">
    <div id="flex-body">
  <div id="site-nav">
    <ul style="list-style-type: none; margin-left:-1em">
      <li style="margin-top: 10px">
        <h2 id="steiners-muse">Steiner's Muse<a class="fa fa-anchor" href="#steiners-muse"></a></h2>
      </li>
      <li style="margin-top: 10px"><a href="/Steiners_Muse/index.html#introduction">Introduction</a></li>
      <li style="margin-top: 10px"><a href="/Steiners_Muse/index.html#background">Background</a></li>
      <li style="margin-top: 10px"><a href="/Steiners_Muse/index.html#the-journey">The Journey</a></li>
      <li style="margin-top: 10px"><a href="/Steiners_Muse/index.html#critical-analysis">Critical Analysis</a></li>
      <li style="margin-top: 10px">
        <br></li>
      <li style="margin-top: 10px">
        <h2 id="media">Media<a class="fa fa-anchor" href="#media"></a></h2>
      </li>
      <li style="margin-top: 10px"><a href="/Steiners_Muse/media/index.html#making-of-steiners-muse">Making of Steiner's Muse</a></li>
      <li style="margin-top: 10px"><a href="/Steiners_Muse/media/index.html#steiners-muse-album">Curated Album</a></li>
      <li style="margin-top: 10px">
        <br></li>
      <li style="margin-top: 10px">
        <h2 id="the-team">The Team<a class="fa fa-anchor" href="#the-team"></a></h2>
      </li>
      <li style="margin-top: 10px"><a href="/Steiners_Muse/the_team/index.html">About Us</a></li>
    </ul>
  </div>
  <div id="site-nav-btn-wrap">
    <div id="site-nav-btn">
      <div class="menu-top-bar"></div>
      <div class="menu-middle-bar"></div>
      <div class="menu-bottom-bar"></div>
    </div>
  </div>
  <div id="page-content">
    <div id="content-wrapper">
      <navbar placement="top" type="inverse">
        <a slot="brand" href="/" title="Home" class="navbar-brand">Project Steiner</a>
        <dropdown text="Sections" class="nav-link">
          <li><a href="/Steiners_Muse/index.html" class="dropdown-item">Steiner's Muse</a></li>
          <li><a href="/Steiners_Muse/media/index.html" class="dropdown-item">Media</a></li>
          <li><a href="/Steiners_Muse/the_team/index.html" class="dropdown-item">About Us</a></li>
        </dropdown>
        <li>
          <a href="https://github.com/Chng-Zhi-Xuan/Steiners_Muse" target="_blank" class="nav-link">
            Link to Github
            <svg height="16px" fill="#777" class="octicon octicon-mark-github" viewbox="0 0 16 16" version="1.1" aria-hidden="true">
              <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16  8c0-4.42-3.58-8-8-8z" /></svg></a></li>
        <li slot="right">
          <searchbar :data="searchData" placeholder="Search Document" :on-hit="searchCallback" menu-align-right=""></searchbar>
        </li>
      </navbar>
      <br><br>
      <div>
        <h1 id="project-steiner-the-search-for-a-creative-agent">Project Steiner : The Search For A Creative Agent<a class="fa fa-anchor" href="#project-steiner-the-search-for-a-creative-agent"></a></h1>
      </div>
      <panel header="## Introduction" type="primary" no-close="" expanded="">
        <div>
          <h3 id="what-is-steiners-muse">What is &quot;Steiner’s Muse&quot;?<a class="fa fa-anchor" href="#what-is-steiners-muse"></a></h3>
        </div>
        “Steiner’s Muse” is a Digital Artifact created for Exploring Computational Media Literacy (GET1033, National University of Singapore) as a final project. Its development took 2 weeks of learning model experimentation and design. After iterating through 4 experimental prototypes, the final product is a Multi-Model Character-based Recurrent Neural Network utilizing <span><strong>L</strong>ong <strong>S</strong>hort <strong>T</strong>erm <strong>M</strong>emory</span> (LSTM).
        <br><br>
        Steiner's Muse is capable of generating unlimited unique piano scores by giving it Musical Instrument Digital Interface (MIDI) files for it to train on. Do note that you should give similar genre MIDI files to it to train for best results. Mixing genres and song tempos will have sequences of conflicting melodies stitched together.
        <br><br>
        <div>
          <h3 id="goal">Goal<a class="fa fa-anchor" href="#goal"></a></h3>
        </div>
        The goal of this project is to explore how music can be procedurally generated by computers without human intervention. We aim to accomplish this by designing a model to learn from existing music compositions via midi files, and thereafter generate its own music.
        <br><br>
        <div>
          <h3 id="acknowledgements">Acknowledgements<a class="fa fa-anchor" href="#acknowledgements"></a></h3>
        </div>
        We would like to thank our professor, (Dr. Quitmeyer, Andrew James) and our tutor (Mr. Ang Yee Hong, Dennis).
        <br><br>
        They have guided us to explore ideas which we are unfamiliar with. Although we are amateurs in machine learning, applying it for creativity is something foreign and non-textbook to us. Working on this project has helped us gain a better understanding of the capabilities of machine learning, not just for data analysis, but for art as well.
        <br><br>
        <div>
          <h3 id="license">License<a class="fa fa-anchor" href="#license"></a></h3>
          <p>Steiner's Muse is under the <a href="https://opensource.org/licenses/MIT">MIT License</a>, full description available <a href="https://github.com/Chng-Zhi-Xuan/Steiners_Muse/blob/master/LICENSE">here</a>. Feel free to use and modify our code to start generating your own music!
            <br><br></p>
        </div>
        <div>
          <h3 id="resources-used">Resources used<a class="fa fa-anchor" href="#resources-used"></a></h3>
          <ul>
            <li>Environment
              <ul>
                <li><a href="https://colab.research.google.com/">Google Collaboratory</a></li>
              </ul>
            </li>
            <li>Libraries
              <ul>
                <li><a href="https://keras.io/">Keras</a></li>
                <li><a href="http://web.mit.edu/music21/">music21</a></li>
                <li><a href="http://www.numpy.org/">NumPy</a></li>
              </ul>
            </li>
            <li>Datasets
              <ul>
                <li><a href="http://www.piano-midi.de/">Piano-Midi</a></li>
                <li><a href="https://www.youtube.com/user/FonziMGM">Fonzi M</a></li>
                <li><a href="https://bushgrafts.com/midi/">Bush Grafts</a></li>
              </ul>
            </li>
            <li>Papers
              <ul>
                <li><a href="https://medium.com/all-things-ai/ai-music-composer-13c8086282c2">Ai music composer</a></li>
                <li><a href="https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5">How to Generate Music using a LSTM Neural Network in Keras</a></li>
                <li><a href="https://cs224d.stanford.edu/reports/allenh.pdf">Deep Learning for Music</a></li>
              </ul>
            </li>
          </ul>
        </div>
      </panel>
      <panel header="## Background" type="info" no-close="" expanded="">
        This section provides the knowledge needed to understand how Stein’s Muse works. We assume a rudimentary understanding of what a midi file is.
        <div>
          <h3 id="processing-midi-files-with-music21">Processing MIDI Files with music21<a class="fa fa-anchor" href="#processing-midi-files-with-music21"></a></h3>
          <p>music21 is a Python library which makes MIDI file processing easy.
            MIDI files consist of sequences of notes with the following basic properties:</p>
          <ol>
            <li>Pitch (What note it is)</li>
            <li>Velocity (How hard a note is pressed)</li>
            <li>Duration (How long a note is played)</li>
            <li>Offset (Where a note is placed in a score)</li>
          </ol>
          <p>music21 also represents notes within the same key as chords, as it reads MIDI files. The following is an excerpt of a MIDI file as seen by music21:
          </p>
        </div>
        <div class="text-center">
          <pic src="/Steiners_Muse/images/music21.png" alt="music21 visual" height="300"></pic>
        </div>
        <br><br>
        <div>
          <h3 id="data-representation">Data Representation<a class="fa fa-anchor" href="#data-representation"></a></h3>
          <p>In our search for a way to describe MIDI files to the model, we have come across a few interesting techniques.
            <br><br>
            <strong>MIDI to Image</strong>
            <br>
            <a href="https://www.youtube.com/watch?v=nA3YOFUCn4U&amp;t=208s">YouTuber carykh</a> had converted the music scores into images. Convolutional neural networks (a form of network which excels at image recognition) were then used to learn an agent to generate music.
            <br><br>
            <strong>Character-based embedding</strong>
            <br>
            By viewing each note and chord as a character, the types of possible outputs are restricted to characters the network has seen before. This involves forming a dictionary of note combinations that work. To represent each character in the network, one-hot encoding is used.</p>
          <div class="text-center">
            <pic src="/Steiners_Muse/images/oneHotEncoding.png" alt="one hot encoding visual" height="400"></pic>
          </div>
          In one-hot encoding, each note is represented by a ‘1’ in its corresponding position. In the above example, ‘C’ is being activated in the embedding. We chose to use character based embedding in this project, to prevent the likelihood of erroneous note combination generation.
        </div>
        <br><br>
        <div>
          <h3 id="neural-networks">Neural Networks<a class="fa fa-anchor" href="#neural-networks"></a></h3>
          <p>Due to the complexity of music, it is not feasible to formulate a generative algorithm. Hence, we have decided to use neural networks, which are inspired by the structure of brains.</p>
          <div class="table-responsive">
            <table class="markbind-table table table-bordered table-striped">
              <thead>
                <tr>
                  <th>
                    <div class="text-center">Network</div>
                  </th>
                  <th>
                    <div class="text-center">Neurons</div>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <pic src="/Steiners_Muse/images/NeuralNetwork.png" alt="Neural Network"></pic>
                  </td>
                  <td>
                    <pic src="/Steiners_Muse/images/Neurons.png" alt="Neurons"></pic>
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>Neural networks consists of an <span class="text-danger">Input Layer</span>, <span class="text-info">Hidden Layers</span>, and an <span class="text-success">Output Layer</span>. Each layer consists of nodes; each node simulates a neuron cell. Descriptions of the dataset are fed through the <span class="text-danger">Input Layer</span>. Each node then processes the inputs and outputs a signal for nodes in the next layer. Signals propagate through the <span class="text-info">Hidden Layers</span>, and finally accumulate at the <span class="text-success">Output Layer</span>, which represents the predictions made by the network.
            <br><br>
            To train a network using supervised learning, targets are provided for each input provided. The goal is for the network’s output to match the targets. Through iterations of training sessions, the network learns to match these targets via gradient descent and backpropagation.
            <br><br>
            In music generation, each input is represented by the notes that are being played at some time x. However, a basic neural network has no ability to remember previous inputs. This presents a problem as notes played at a point in time are dependent on the previous notes played.
            <br><br>
            Thankfully, recurrent neural networks exist.</p>
          <div class="text-center">
            <pic src="/Steiners_Muse/images/RecurrentNeuralNetwork.png" alt="one hot encoding visual" height="200"></pic>
          </div>
          Simply put, previous outputs are taken into account as they are fed back into the network. In particular, LSTMs, a type of recurrent neural network, excels at remembering histories. In this project, we have implemented our model with LSTMs.
        </div>
      </panel>
      <panel header="## The Journey" type="success" no-close="" expanded="">
        This section documents the insights gained during prototyping, and the relevant transformations our model undertook.
        <br><br>
        <div>
          <h3 id="steiners-muse-prototype-1-note-based-lstm">Steiner’s Muse Prototype 1 : Note-based LSTM<a class="fa fa-anchor" href="#steiners-muse-prototype-1-note-based-lstm"></a></h3>
          <p>The 1st prototype was used as a base to help us gain a better understanding of the task we are undertaking. “Ai music composer” by <a href="https://medium.com/all-things-ai/ai-music-composer-13c8086282c2">Manish Pawar</a> was used as a guide.
            <br></p>
          <p><strong>How does it work</strong>
            <br>
            The model reads sequences of notes in the following manner:</p>
          <div class="text-center">
            <pic src="/Steiners_Muse/images/noteLSTM_model.png" alt="one hot encoding visual" height="200"></pic>
          </div>
          Notes were represented as such : [Pitch, Velocity, Offset]. No form of embedding was used to represent the dataset. The descriptions were represented as float values, which allowed the model to output a note with any pitch, velocity, and offset.
          The following is an example of the input the model would receive:
          <div class="text-center">
            <pic src="/Steiners_Muse/images/noteLSTM_window.png" alt="one hot encoding visual" height="400"></pic>
            <br>
            Window refers to the space in time the model has access to.
          </div>
          <br>
          <p><strong>Insights Gained</strong>
            <br>
            One would expect that the flexibility provided by this form of dataset representation, enables the model to express more creativity. However, that also meant that there were no rules or restrictions set in place to help guide the model to play notes that would sound coherent together. As expected, it resulted in sequences of notes that did not make sense together.
            <br><br>
            Also, due to the way the notes were read, there was no distinguishable accompaniment and melody in the generated music.
            <br><br>
            <strong>Music Generated</strong>
            <br></p>
          <iframe width="100%" height="300" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https://api.soundcloud.com/tracks/530165130&color=#ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true&visual=true"></iframe>
          <br>
          The music generated was terrifying. However, with the lessons learned, we moved on to our next prototype.
        </div>
        <br><br>
        <div>
          <h3 id="steiners-muse-prototype-2-character-based-lstm">Steiner’s Muse Prototype 2 : Character-based LSTM<a class="fa fa-anchor" href="#steiners-muse-prototype-2-character-based-lstm"></a></h3>
          <p>The 2nd prototype was inspired by “How to Generate Music using a LSTM Neural Network in Keras” by <a href="https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5">Sigurour Skuli</a>.
            <br></p>
          <p><strong>How does it work</strong>
            <br>
            Unlike the 1st prototype, the 2nd prototype reads notes in the following manner:</p>
          <div class="text-center">
            <pic src="/Steiners_Muse/images/charLSTM_model.png" alt="one hot encoding visual" height="200"></pic>
          </div>
          When 2 or more notes occur in the same time step and belong to the same key, they are represented as chords. Also, unlike the 1st prototype, only the pitch of a note is described (chords are made of notes). All chords and notes are then represented as characters in a dictionary, and its one-hot encoding is also compiled.
          The following is an example of the input the model would receive (not in actual scale):
          <div class="text-center">
            <pic src="/Steiners_Muse/images/charLSTM_window.png" alt="one hot encoding visual" height="400"></pic>
            <br>
            Difference being, velocity and offset of the note was fixed in this model.
          </div>
          <br>
          <p><strong>Insights Gained</strong>
            <br>
            Prototype 2 is able to generate much more coherent music as compared to prototype 1. By creating a dictionary of possible notes and chords, the model managed to generate notes within the same key, hence resulting in music that did not sound nonsensical.
            <br><br>
            However, there were shortcomings to the model. As the velocity was fixed, there was no sense of accompaniment and melody due to the same volumes for every note. There was also a loss of emotion in the generated music, which is often provided by volume variation. Also, due to the fixed offset, there was no variation in tempo for the music generated. This resulted in mundane music that at least made sense.
            <br></p>
          <p><strong>Music Generated</strong>
            <br></p>
          <iframe width="100%" height="300" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https://api.soundcloud.com/playlists/644294589&color=#ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true&visual=true"></iframe>
          <br>
          Here is a sample of the music generated by this model.
        </div>
        <br><br>
        <div>
          <h3 id="steiners-muse-prototype-25-character-based-with-features-lstm">Steiner’s Muse Prototype 2.5 : Character-based with Features LSTM<a class="fa fa-anchor" href="#steiners-muse-prototype-25-character-based-with-features-lstm"></a></h3>
          <p>In an attempt to overcome the shortcomings in prototype 2, prototype 2.5 was designed to include velocity, offset and duration variation.
            <br></p>
          <p><strong>How does it work</strong>
            <br>
            The input for this model is similar to the 2nd prototype. However, in this model, the note and chords velocity and velocities were read as part of their descriptions too. This resulted in an exponential increase in dictionary size.
            <br></p>
          <p><strong>Insights Gained</strong>
            <br>
            Unfortunately, due to the exponential increase in dictionary size, prototype 2.5 could not be executed. We had hit a roadblock as there was insufficient computer power available to run a project of this scale.
            <br><br>
            Another creative method for our model to learn note transitions, velocity variations, and timing (combination of offset and duration) designs was required if we were to generate better music.
          </p>
        </div>
        <div>
          <h3 id="steiners-muse-final-prototype-3-multi-model-character-based-lstm">Steiner’s Muse Final Prototype 3 : Multi-model Character-based LSTM<a class="fa fa-anchor" href="#steiners-muse-final-prototype-3-multi-model-character-based-lstm"></a></h3>
          <p>The inspiration for the final prototype is simple : why restrict ourselves to one brain, when we can have multiple brains working on separate tasks.
            <br></p>
          <p><strong>How does it work</strong>
            <br>
            Exactly like it sounds, the final prototype consists of multiple models to generate music:</p>
          <ol>
            <li>Note model</li>
            <li>Timing model (read as [offset, duration])</li>
            <li>Velocity model</li>
          </ol>
          <p>A sample input for each of the models is described as such (not in actual scale):</p>
          <div class="text-center">
            <pic src="/Steiners_Muse/images/multiModelLSTM_model.png" alt="one hot encoding visual"></pic>
            <br>
            The outputs from each model are combined to form the generated music.
          </div>
          <br>
          <p><strong>Insights Gained</strong>
            <br>
            Splitting the problem up for multiple networks proved to be a great idea for reducing the computational power required. Although the results are not as great as other state-of-the-art music generators out there, we feel that there has been tremendous progress for 2 weeks.
            <br></p>
          <p><strong>Music Generated</strong>
            <br></p>
          <iframe width="100%" height="300" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https://api.soundcloud.com/playlists/641063427&color=#ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true&visual=true"></iframe>
          <br>
          Here is a sample of the music generated by Steiner’s Muse.
          <br><br>
        </div>
        <div>
          <h3 id="other-challenges-faced-future-iterations">Other Challenges Faced / Future Iterations<a class="fa fa-anchor" href="#other-challenges-faced-future-iterations"></a></h3>
          <p>Due to the limited time we have, we could not thoroughly curate the datasets for training. In our experience with this project, we have noticed that it is important to ensure that the music used in each training session are similar in genre, speed, key type (major/minor), etc. There may also be many other confounding variables we are not familiar with since we do not have an in-depth knowledge of music composition.
            <br><br>
            As such, each training session was executed with a dataset that has a maximum of 20 songs. This restricts the knowledge our model can learn, and hence impacts the variation of the music generated.
            <br><br>
            In future iterations, we hope to be able to prepare larger datasets for the model to train with.
            <br><br>
            Also, the current model is limited to piano instrumentals only. It would be interesting to train the model on multi-instrumental compositions, and allow the model to compose orchestras. However, the undertaking of such a task requires large computational power, which is currently a restriction.</p>
        </div>
      </panel>
      <panel header="## Critical Analysis" type="warning" no-close="" expanded="">
        <div>
          <h3 id="motivation-for-project-steiner">Motivation for Project Steiner<a class="fa fa-anchor" href="#motivation-for-project-steiner"></a></h3>
          <p>Steiner’s Muse experimentation and journey was motivated by GET1033 course material, in particular Machine Learning (ML) and Artificial Intelligence (AI). Most of today’s application in ML and AI is cold and practical, commonly used to solve business problems, in the form of Big Data and Data Analytics.
            <br><br>
            With inspiration from <a href="http://andy.dorkfort.com/papers/X%20QUITMEYER.pdf">Dr Quitmeyer’s work</a>, which has a common theme of unifying technology with nature, Steiner’s Muse was born. We wanted to introduce a humanistic side to computers. Hence Project Steiner was initiated to have a computer emulate human creativity by generating unique music.
            <br><br>
            Generating music using ML is not a new concept. Marvelous computer composers such as Emily Howell (2009) are able to generate piano pieces, whilst recently, Aiva (2016) is able to generate full scale orchestras.
            <br><br>
            Project Steiner aims to explore the limits of generating music of different genres (jazz, classical, video game, anime etc.) using ML.</p>
        </div>
        <div>
          <h3 id="making-use-of-computer-advancements">Making use of Computer Advancements<a class="fa fa-anchor" href="#making-use-of-computer-advancements"></a></h3>
          <p>The heart of Steiner’s Muse is the LSTM Neural Network, which in the recent decade has exploded in popularity and provides us with an easily accessible generative medium. This is largely due to three main factors:</p>
          <ol>
            <li>High quality digital data that are available for public (Big data)</li>
            <li>Increased computation power in processors (Reduced training time)</li>
            <li>Intuitive, easy to use and free Machine Learning frameworks (Tensorflow / Keras)</li>
          </ol>
          <p>In the world of music composition, digital synthesizers has also evolved along with computers. Closing the gap between simulated instrument sounds and an actual recording. Thus composing music isn’t bounded by high entry barriers, anybody could download software and be a composer themselves.
            <br><br>
            Additionally, synthesizer music files (MIDI format) are a file type that allows computer programs to read scores as bits. This allows us to be able to work with music as data, which can then be used in ML. Combining all the points mentioned, the creation of a program to generate unique pieces of music isn’t science fiction but reality.
            <br><br>
            Steiner’s Muse can not only generate unique pieces for a specific genre, its volume is also unlimited. A human composer may take their entire lives to generate one week’s duration of musical pieces, Steiner’s Muse only needs a few minutes to do the same.
          </p>
        </div>
        <div>
          <h3 id="copyright-implications-in-computer-generated-content">Copyright Implications in Computer Generated Content<a class="fa fa-anchor" href="#copyright-implications-in-computer-generated-content"></a></h3>
          <p>The module’s course material also covered digital intellectual property, where existing copyright laws can’t keep up with technological advancements. This creates grey areas on deciding who owns the copyright for machines that generate copyrightable content.
            <br><br>
            A prerequisite for Steiner’s Muse to learn how to compose music is having existing compositions to train on. If a person wrote his own songs, programmed his own song generator and used his songs for generating new songs. Then naturally, the person owns the songs generated.
            <br><br>
            However, what if the song owner, song generator programmer and user are all different people? Then the following questions might not have an answer at all:</p>
          <ol>
            <li>Who owns the copyright of the generated music?</li>
            <li>Is using copyrighted music for training the song generator considered “Fair Use”?</li>
          </ol>
          <p>Thus in such cases, it may be best to take a policy of least resistance. Where nobody can own the copyright of the computer generated song, nor commercialize it when there are conflicting parties.
          </p>
        </div>
        <div>
          <h3 id="the-4-affordances-in-steiners-muse">The 4 affordances in Steiner’s Muse<a class="fa fa-anchor" href="#the-4-affordances-in-steiners-muse"></a></h3>
          <p>This is a representation methodology for any digital format or genre, inspired from <a href="https://inventingthemedium.com/four-affordances/">Janet H. Murray</a></p>
          <p><strong>Procedural</strong>
            <br>
            Within Steiner’s Muse, the procedural aspect might seem low as we are only able to perceive the generated music. However, to gain the capabilities to generate the music, Steiner’s Muse first has to form its own understanding of music composition. After giving it a dataset to study, Steiner’s Muse follows a sequence of rules for learning (a series of calculations). After Steiner’s Muse has created its own judgements on music, it can now follow its own set of rules to generate its own music, when given a sample of music to kickstart the process. Hence, the procedural aspect is high in Steiner’s Muse.
            <br></p>
          <p><strong>Participatory</strong>
            <br>
            A typical user need only input MIDI files for training and then listen to the music generated (Low). If the output is not to his liking, then the user may need to curate similar sounding songs for Steiner’s Muse to train more consistently (Medium).
            <br><br>
            A advanced user (with knowledge in Machine Learning) can have much higher elevated participation by tuning Steiner’s Muse internal network structure to change the way the music is generated (High). Thus, the participatory affordance varies from low to high depending on the type of user.
            <br></p>
          <p><strong>Encyclopedic</strong>
            <br>
            Steiner’s Muse can generate an endless sequence of music by itself. But in order to do so, it has to process the input into different parts such as notes, timing and velocity. All of this processed data is stored into dictionaries for Steiner’s Muse to refer to. Additionally, there is descriptive console text that shows the user the progress of training and a visualization of the processed data. Thus the encyclopedic affordance is high.
            <br></p>
          <p><strong>Spatial</strong>
            <br>
            The interaction between Steiner’s Muse and the user is just a console. There isn’t much navigation of information or virtual space at all other than visualizing raw or processed input data. Thus the spatial affordance is low.
          </p>
        </div>
      </panel>
      <br><br>
    </div>
  </div>
</div>
<div id="flex-div"></div>
<footer class="text-center bg-dark text-warning">
  <span>⏳</span> El Psy Congroo <span>🍌</span>
  <br>
  <small class="text-white">[Generated using <a href="https://markbind.github.io/markbind/">MarkBind 1.14.0</a>]<small></small></small></footer>
</div>
</body>
<script src="markbind\js\vue.min.js"></script>
<script src="markbind\js\vue-strap.min.js"></script>
<script src="markbind\js\polyfill.min.js"></script>
<script src="markbind\js\bootstrap-vue.min.js"></script>
<script>
    const baseUrl = '/Steiners_Muse'
</script>
<script src="markbind\js\setup.js"></script>
<script src="markbind\layouts\default\scripts.js"></script>
</html>
